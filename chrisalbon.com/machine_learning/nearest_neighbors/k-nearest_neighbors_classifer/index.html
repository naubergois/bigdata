<!DOCTYPE html>
<html lang="en">

<head>

    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66582-32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66582-32');
    </script>

    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="K-Nearest Neighbors Classification" />
<meta property="og:description" content="A quick guide to using k-nearest neighbor using numpy and scikit." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://chrisalbon.com/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/" />
<meta property="article:published_time" content="2017-12-20T11:53:49-07:00"/>
<meta property="article:modified_time" content="2017-12-20T11:53:49-07:00"/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="K-Nearest Neighbors Classification"/>
<meta name="twitter:description" content="A quick guide to using k-nearest neighbor using numpy and scikit."/>
<meta name="generator" content="Hugo 0.54.0" /> 
    
    
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BlogPosting",
  "headline": "K-Nearest Neighbors Classification",
  "url": "https://chrisalbon.com/machine_learning/nearest_neighbors/k-nearest_neighbors_classifer/",
  "wordCount": "626",
  "datePublished": "2017-12-20T11:53:49-07:00",
  "dateModified": "2017-12-20T11:53:49-07:00",
  "author": {
    "@type": "Person",
    "name": "Chris Albon"
  },
  "description": "A quick guide to using k-nearest neighbor using numpy and scikit."
}
</script> 

    <title>K-Nearest Neighbors Classification</title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
        crossorigin="anonymous">

    
    <link href="https://chrisalbon.com/css/custom.css" rel="stylesheet"> 
    <link href="https://chrisalbon.com/css/syntax.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,500,700" rel="stylesheet">
        
    
    <link href="" rel="alternate" type="application/rss+xml" title="Chris Albon" /> 
    
    <link href="https://chrisalbon.com/#articles" rel="alternate" type="application/rss+xml" title="Leadership posts" />
    
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[','\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

</head>

<body>

    <nav class="navbar navbar-expand-sm fixed-top">
        <div class="container">
            <a class="navbar-brand" href="https://chrisalbon.com/">Chris Albon</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="nav navbar-nav mr-auto"></ul>
                <ul class="navbar-nav">

                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true"
                            aria-expanded="false">
                            Technical Notes
                        </a>
                        <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="https://chrisalbon.com/#machine_learning">Machine Learning</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#deep_learning">Deep Learning</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#python">Python</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#statistics">Statistics</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#scala">Scala</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#snowflake">Snowflake</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#postgresql">PostgreSQL</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#linux">Command Line</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#regex">Regular Expressions</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#mathematics">Mathematics</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#aws">AWS</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#git_and_github">Git &amp; GitHub</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#computer_science">Computer Science</a>
                        </div>
                    </li>
                    
                    
                    
                    <li class="nav-item">
                        <a class="nav-link" href="https://chrisalbon.com/#articles">Articles</a>
                    </li>
                    
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true"
                            aria-expanded="false">
                            About
                        </a>
                        <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                            <a class="dropdown-item" href="https://chrisalbon.com/about/chris_albon/">About Chris</a>
                            <a class="dropdown-item" href="https://github.com/chrisalbon">GitHub</a>
                            <a class="dropdown-item" href="https://twitter.com/chrisalbon">Twitter</a>
                            <a class="dropdown-item" href="https://www.amazon.com/Machine-Learning-Python-Cookbook-Preprocessing/dp/1491989386">ML Book</a>
                            <a class="dropdown-item" href="https://machinelearningflashcards.com/">ML Flashcards</a>
                            <a class="dropdown-item" href="https://chrisalbon.com/#">Newsletter</a>
                        </div>
                    </li>
                </ul>
            </div>
        </div>
    </nav>


    
    <div class="container">
        <div class="row">
            <div class="col-sm-12">

                 


<article>
  <div class="technical_note">
  <header>
      <div class="alert alert-warning flashcard_ad" role="alert">
          Learning machine learning? Try my <a href="https://machinelearningflashcards.com" class="alert-link">machine learning flashcards</a> or <a href='https://amzn.to/2HwnWty' class="alert-link">Machine Learning with Python Cookbook</a>.
      </div>
    <h1 class="technical_note_title">K-Nearest Neighbors Classification</h1>
    <div class="technical_note_date">
      <time datetime=" 2017-12-20T11:53:49-07:00 "> 20 Dec 2017</time>
    </div>
  </header>
  <div class="content">
      
  

<p>K-nearest neighbors classifier (KNN) is a simple and powerful classification learner.</p>

<p>KNN has three basic parts:</p>

<ul>
<li>$y_i$: The class of an observation (what we are trying to predict in the test data).</li>
<li>$X_i$: The predictors/IVs/attributes of an observation.</li>
<li>$K$: A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its &ldquo;neighborhood&rdquo;. For example, K=2 means that each observation&rsquo;s has a neighorhood comprising of the two other observations closest to it.</li>
</ul>

<p>Imagine we have an observation where we know its independent variables $x_{test}$ but do not know its class $y_{test}$. The KNN learner finds the K other observations that are closest to $x_{test}$ and uses their known classes to assign a classes to $_{test}$.</p>

<h2 id="preliminaries">Preliminaries</h2>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">neighbors</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>  
<span class="kn">import</span> <span class="nn">seaborn</span></code></pre></div>
<h2 id="create-dataset">Create Dataset</h2>

<p>Here we create three variables, <code>test_1</code> and <code>test_2</code> are our independent variables, &lsquo;outcome&rsquo; is our dependent variable. We will use this data to train our learner.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">training_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;test_1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3051</span><span class="p">,</span><span class="mf">0.4949</span><span class="p">,</span><span class="mf">0.6974</span><span class="p">,</span><span class="mf">0.3769</span><span class="p">,</span><span class="mf">0.2231</span><span class="p">,</span><span class="mf">0.341</span><span class="p">,</span><span class="mf">0.4436</span><span class="p">,</span><span class="mf">0.5897</span><span class="p">,</span><span class="mf">0.6308</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]</span>
<span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;test_2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5846</span><span class="p">,</span><span class="mf">0.2654</span><span class="p">,</span><span class="mf">0.2615</span><span class="p">,</span><span class="mf">0.4538</span><span class="p">,</span><span class="mf">0.4615</span><span class="p">,</span><span class="mf">0.8308</span><span class="p">,</span><span class="mf">0.4962</span><span class="p">,</span><span class="mf">0.3269</span><span class="p">,</span><span class="mf">0.5346</span><span class="p">,</span><span class="mf">0.6731</span><span class="p">]</span>
<span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;win&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>

<span class="n">training_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></code></pre></div>
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test_1</th>
      <th>test_2</th>
      <th>outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.3051</td>
      <td>0.5846</td>
      <td>win</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.4949</td>
      <td>0.2654</td>
      <td>win</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.6974</td>
      <td>0.2615</td>
      <td>win</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.3769</td>
      <td>0.4538</td>
      <td>win</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.2231</td>
      <td>0.4615</td>
      <td>win</td>
    </tr>
  </tbody>
</table>
</div>

<h2 id="plot-the-data">Plot the data</h2>

<p>This is not necessary, but because we only have three variables, we can plot the training dataset. The X and Y axes are the independent variables, while the colors of the points are their classes.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">seaborn</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="s1">&#39;test_1&#39;</span><span class="p">,</span> <span class="s1">&#39;test_2&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">training_data</span><span class="p">,</span> <span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="s2">&#34;outcome&#34;</span><span class="p">,</span> <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&#34;marker&#34;</span><span class="p">:</span> <span class="s2">&#34;D&#34;</span><span class="p">,</span><span class="s2">&#34;s&#34;</span><span class="p">:</span> <span class="mi">100</span><span class="p">})</span></code></pre></div>
<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x11008aeb8&gt;
</code></pre>

<p><img src="k-nearest_neighbors_classifer_9_1.png" alt="png" /></p>

<h2 id="convert-data-into-np-arrays">Convert Data Into np.arrays</h2>

<p>The <code>scikit-learn</code> library requires the data be formatted as a <code>numpy</code> array. Here are doing that reformatting.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X</span> <span class="o">=</span> <span class="n">training_data</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;test_1&#39;</span><span class="p">,</span> <span class="s1">&#39;test_2&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_data</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">])</span></code></pre></div>
<h2 id="train-the-learner">Train The Learner</h2>

<p>This is our big moment. We train a KNN learner using the parameters that an observation&rsquo;s neighborhood is its three closest neighors. <code>weights = 'uniform'</code> can be thought of as the voting system used. For example, <code>uniform</code> means that all neighbors get an equally weighted &ldquo;vote&rdquo; about an observation&rsquo;s class while <code>weights = 'distance'</code> would tell the learner to weigh each observation&rsquo;s &ldquo;vote&rdquo; by its distance from the observation we are classifying.</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">clf</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
<span class="n">trained_model</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<h2 id="view-the-model-s-score">View The Model&rsquo;s Score</h2>

<p>How good is our trained model compared to our training data?</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">trained_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code></pre></div>
<pre><code>0.80000000000000004
</code></pre>

<p>Our model is 80% accurate!</p>

<p><em>Note: that in any real world example we&rsquo;d want to compare the trained model to some holdout test data. But since this is a toy example I used the training data</em>.</p>

<h2 id="apply-the-learner-to-a-new-data-point">Apply The Learner To A New Data Point</h2>

<p>Now that we have trained our model, we can predict the class any new observation, $y_{test}$. Let us do that now!</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Create a new observation with the value of the first independent variable, &#39;test_1&#39;, as .4 </span>
<span class="c1"># and the second independent variable, test_1&#39;, as .6 </span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">.</span><span class="mi">4</span><span class="p">,</span><span class="o">.</span><span class="mi">6</span><span class="p">]])</span></code></pre></div><div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Apply the learner to the new, unclassified observation.</span>
<span class="n">trained_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span></code></pre></div>
<pre><code>array(['loss'], dtype=object)
</code></pre>

<p>Huzzah! We can see that the learner has predicted that the new observation&rsquo;s class is <code>loss</code>.</p>

<p>We can even look at the probabilities the learner assigned to each class:</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">trained_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span></code></pre></div>
<pre><code>array([[ 0.66666667,  0.33333333]])
</code></pre>

<p>According to this result, the model predicted that the observation was <code>loss</code> with a ~67% probability and <code>win</code> with a ~33% probability. Because the observation had a greater probability of being <code>loss</code>, it predicted that class for the observation.</p>

<h2 id="notes">Notes</h2>

<ul>
<li>The choice of K has major affects on the classifer created.</li>
<li>The greater the K, more linear (high bias and low variance) the decision boundary.</li>
<li>There are a variety of ways to measure distance, two popular being simple euclidean distance and cosine similarity.</li>
</ul>

</div>
  <aside>
      <div class="bug_reporting">
          <h4>Find an error or bug?</h4>
          <p>Everything on this site is available on GitHub. Head to <a href='https://github.com/chrisalbon/notes/issues/new'>and submit a suggested change</a>. Include the tutorial's URL in the issue.</p>
      </div>
      </aside>

    </div>
</article>




            </div>

        </div>
    </div>

    

    <footer class="footer text-center">
        <div class="container">
            <span class="text-muted">Copyright &copy; Chris Albon, <time datetime="2020">2020</time>. All 622 notes and articles are available on <a href="https://github.com/chrisalbon/notes">GitHub</a>.</span>
        </div>
    </footer>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ"
        crossorigin="anonymous"></script>

</body>

</html>