{"cells":[{"cell_type":"markdown","source":["In our notebook we have access to sqlContext"],"metadata":{}},{"cell_type":"code","source":["sqlContext"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: &lt;pyspark.sql.context.HiveContext at 0x7fccd1a030b8&gt;</div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["Spark Context"],"metadata":{}},{"cell_type":"code","source":["sc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.252.167:43608\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":4},{"cell_type":"markdown","source":["And SparkSession"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://10.172.252.167:43608\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":6},{"cell_type":"markdown","source":["We can use the Spark Context to create a small python range that will provide a return type of `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["firstDataFrame = spark.range(1000000)\nprint(firstDataFrame)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">DataFrame[id: bigint]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["You might have expected the last command to print the values of `firstDataFrame`.  But the `range` command is a **transformation** operation, and Spark will wait until the values are needed before it calculates them.  Spark will know a value is needed when you execute an **action** operation.  \n\n`show()` is an **action**:"],"metadata":{}},{"cell_type":"code","source":["firstDataFrame.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+\n id|\n+---+\n  0|\n  1|\n  2|\n  3|\n  4|\n  5|\n  6|\n  7|\n  8|\n  9|\n 10|\n 11|\n 12|\n 13|\n 14|\n 15|\n 16|\n 17|\n 18|\n 19|\n+---+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["To show you the values in the DataFrame, Spark must know the values first.  When you call `show()` you force Spark to execute all the transformations needed in order to know the values in the DataFrame, and Spark will automatically do this at the right time.\n\nHere are some simple examples of transformations and actions.\n\nTransformations (lazy) - select, distinct, groupBy, sum, orderBy, filter, limit\nActions - show, count, collect, save"],"metadata":{}},{"cell_type":"markdown","source":["Try another transformation.  Tell Spark that we want to create a `secondDataFrame` by multiplying 2 by each value in the `firstDataFrame`."],"metadata":{}},{"cell_type":"code","source":["# select the ID column values and multiply them by 2\nsecondDataFrame = firstDataFrame.selectExpr(\"(id * 2) as value\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["`take` is another action.  This action selects a certain number of elements from the beginning of a dataframe.  Try to take five values from each DataFrame."],"metadata":{}},{"cell_type":"code","source":["# take the first 5 values that we have in our firstDataFrame, use print to see the results\nprint(firstDataFrame.take(5))\n# take the first 5 values that we have in our secondDataFrame, use print to see the results\nprint(secondDataFrame.take(5))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n[Row(value=0), Row(value=2), Row(value=4), Row(value=6), Row(value=8)]\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["Spark separates operations into **transformations** (which are evaluated only when needed) and **actions** (which are evaluated immediately) to support an optimization called pipelining.  This lets you decide what you want the data to look like, while leaving it up to Spark to find the most efficient way to calculate your results.\n\n![transformations and actions](http://training.databricks.com/databricks_guide/gentle_introduction/pipeline.png)"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"4-TransformationActions","notebookId":3860303934512302},"nbformat":4,"nbformat_minor":0}
